{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/has-c/UserMobilityMining/blob/master/Gowalla%20ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "AniSM0JZu5fr",
    "outputId": "0b37c1aa-6145-461a-ed36-5d47d0275141"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datetime\n",
    "\n",
    "#disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignPastCurrentPOI(checkins, dateList):\n",
    "\n",
    "    #group by userId, then checkin date and then order by time \n",
    "    uniqueUserIds = np.unique(checkins['userid'])\n",
    "    uniqueCheckinDates = sorted(np.unique(dateList))\n",
    "\n",
    "    sortedCheckins = pd.DataFrame([])\n",
    "\n",
    "    for userId in uniqueUserIds:\n",
    "        for checkinDate in uniqueCheckinDates:\n",
    "            \n",
    "            #group by userId and checkin date \n",
    "            groupedByUserId = checkins[userId==checkins['userid']]\n",
    "            groupedByCheckinDate = groupedByUserId[groupedByUserId['Date'] == checkinDate]\n",
    "            if groupedByCheckinDate.shape[0] == 0:\n",
    "                continue\n",
    "                \n",
    "            #sort by time \n",
    "            groupedByCheckinDate.sort_values(by ='Time',ascending=True, inplace=True)\n",
    "            groupedByCheckinDate.index = range(0, groupedByCheckinDate.shape[0])\n",
    "            \n",
    "            #create list of sorted checkins \n",
    "            sortedCheckins = pd.concat([sortedCheckins, groupedByCheckinDate], axis=0, ignore_index=True)\n",
    "    \n",
    "    #align past and current checkins - features and labels entire df\n",
    "    pastCurrentCheckins = pd.DataFrame([])\n",
    "\n",
    "    for userId in uniqueUserIds:\n",
    "        \n",
    "        #group by userId\n",
    "        groupedByUserId = sortedCheckins[userId==checkins['userid']]\n",
    "        \n",
    "        for rowIndex in groupedByUserId.index:\n",
    "            try:\n",
    "                pastPOI = groupedByUserId.loc[rowIndex]\n",
    "                currentPOI = groupedByUserId.loc[rowIndex+1]\n",
    "                poiInstance = list(currentPOI) + list(pastPOI)\n",
    "                pastCurrentCheckins = pd.concat([pastCurrentCheckins, pd.DataFrame(poiInstance).T], axis=0)\n",
    "            except KeyError:\n",
    "                break\n",
    "\n",
    "    #rename fields \n",
    "    pastCurrentCheckinDfColumnNames = ['Current ' + name for name in sortedCheckins.columns] + ['Past ' + name for name in sortedCheckins.columns]\n",
    "    pastCurrentCheckins.columns = pastCurrentCheckinDfColumnNames\n",
    "\n",
    "    return pastCurrentCheckins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "<ul>\n",
    "    <li> One hot encoding </li>\n",
    "    <li> Standard scaling </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "checkinsDf = pd.read_csv(\"/home/usermobilitymining/Notebooks/volume/Hasnain/Processed Data/processedCheckins.csv\")\n",
    "nzCheckins = pd.read_csv(\"/home/usermobilitymining/Notebooks/volume/Hasnain/Processed Data/nzCheckinsWithGridTokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join checkins df and nz checkins to only extract NZ checkins \n",
    "checkinsDf['id'] = checkinsDf.index #create id field\n",
    "nzCheckins.drop(\"lat\", axis=1, inplace=True) #drop lat so no error\n",
    "nzCheckins.drop(\"lng\", axis=1, inplace=True) #drop lng so no error\n",
    "checkinsDf = nzCheckins.join(checkinsDf.set_index('id'), on='id')\n",
    "checkinsDf.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date and time to datetime variables\n",
    "#extract day of the week and hour of checkin\n",
    "checkinDate = [datetime.datetime.strptime(date, \"%Y-%m-%d\").date() for date in checkinsDf['Date']]\n",
    "checkinDay = [date.weekday() for date in checkinDate]\n",
    "checkinTime = [datetime.datetime.strptime(time, '%H:%M:%S').time() for time in checkinsDf['Time']]\n",
    "checkinHour = [time.hour for time in checkinTime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the time variable\n",
    "ohe = OneHotEncoder(dtype=np.int8, n_values=24)\n",
    "hourOfCheckinEncoded = ohe.fit_transform(np.array(checkinHour).reshape(-1,1))\n",
    "timeColumns = ['Time ' + str(num) for num in np.arange(0,24)]\n",
    "timeDf = pd.DataFrame(hourOfCheckinEncoded.toarray(), columns=timeColumns)\n",
    "\n",
    "#one hot encode the day of the week variable\n",
    "ohe = OneHotEncoder(dtype=np.int8, n_values=7)\n",
    "dayOfTheWeekEncoded = ohe.fit_transform(np.array(checkinDay).reshape(-1,1))\n",
    "dayColumns = ['DayOfWeek ' + str(num) for num in np.arange(0,7)]\n",
    "dayDf = pd.DataFrame(dayOfTheWeekEncoded.toarray(), columns=dayColumns)\n",
    "\n",
    "#one hot encode the main category\n",
    "numberOfMainCategories = len(np.unique(checkinsDf['Main Category']))\n",
    "labelEncoder = LabelEncoder()\n",
    "mainCategoriesEncoded = labelEncoder.fit_transform(checkinsDf['Main Category'].values.reshape(-1,1))\n",
    "ohe = OneHotEncoder(dtype=np.int8, n_values=numberOfMainCategories)\n",
    "mainCategoriesEncoded = ohe.fit_transform(labelEncodedCategories.reshape(-1,1))\n",
    "categoryDf = pd.DataFrame(mainCategoriesEncoded.toarray(), columns=labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat fields\n",
    "checkinsDf = pd.concat([checkinsDf, timeDf], axis=1,copy=False) \n",
    "checkinsDf = pd.concat([checkinsDf, dayDf], axis=1,copy=False) \n",
    "checkinsDf = pd.concat([checkinsDf, categoryDf], axis=1,copy=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop fields \n",
    "checkinsDf.drop(\"Date\", axis=1, inplace=True)\n",
    "checkinsDf.drop(\"Time\", axis=1, inplace=True)\n",
    "checkinsDf.drop(\"Main Category\", axis=1, inplace=True)\n",
    "checkinsDf.drop(\"Locations\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split - normal \n",
    "labelDf = checkinsDf[['Community','Entertainment','Food','Nightlife','Outdoors','Shopping','Travel']]\n",
    "featuresDf = checkinsDf.drop(['Community','Entertainment','Food','Nightlife','Outdoors','Shopping','Travel'], axis=1)\n",
    "\n",
    "#train test split\n",
    "featuresTrain, featuresTest, labelTrain, labelTest = train_test_split(featuresDf, labelDf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9K3b-Q9vadV"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(35, input_dim=45, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDAO7Colvaj2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  256/16160 [..............................] - ETA: 39s - loss: 3194817.9062 - accuracy: 0.0859 "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-a65b59d612ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usermobilitymining/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3275\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3277\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usermobilitymining/.local/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'a'"
     ]
    }
   ],
   "source": [
    "#fit model \n",
    "history = model.fit(featuresTrain, labelTrain, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqICs5YLvaRS"
   },
   "outputs": [],
   "source": [
    "#predict\n",
    "labelPredicted = model.predict(featuresTest)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Gowalla ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
